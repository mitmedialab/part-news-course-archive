<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>illustration &#8211; Future of News and Participatory Media</title>
	<atom:link href="https://partnews.mit.edu/tag/illustration/feed/" rel="self" type="application/rss+xml" />
	<link>https://partnews.mit.edu</link>
	<description>Treating newsgathering as an engineering problem... since 2012!</description>
	<lastBuildDate>Wed, 01 Apr 2015 13:21:20 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2</generator>
	<item>
		<title>(How) Can Algorithms be Racist?</title>
		<link>https://partnews.mit.edu/2015/04/01/how-can-algorithms-be-racist/</link>
				<comments>https://partnews.mit.edu/2015/04/01/how-can-algorithms-be-racist/#comments</comments>
				<pubDate>Wed, 01 Apr 2015 05:38:40 +0000</pubDate>
		<dc:creator><![CDATA[Sophie Chou]]></dc:creator>
				<category><![CDATA[Explainers]]></category>
		<category><![CDATA[illustration]]></category>

		<guid isPermaLink="false">http://partnews.brownbag.me/?p=6742</guid>
				<description><![CDATA[Technology can be the ultimate equalizer: once access is provided, it can erase borders, education, race, class. But a new study offers that the same tools that are said to provide a level playing field might also be blind spots.  Are the algorithms that &#8230; <a href="https://partnews.mit.edu/2015/04/01/how-can-algorithms-be-racist/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
								<content:encoded><![CDATA[<p><em>Technology can be the ultimate equalizer: once access is provided, it can erase borders, education, race, class. But a new study offers that the same tools that are said to provide a level playing field might also be blind spots.  Are the algorithms that are used to drive images and ads perpetuating human prejudices?  One study says yes. But, how can algorithms (which seem to be based on reason) discriminate?</em></p>
<blockquote class="instagram-media" data-instgrm-captioned data-instgrm-version="4" style=" background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:584px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);">
<div style="padding:8px;">
<div style=" background:#F8F8F8; line-height:0; margin-top:40px; padding:50% 0; text-align:center; width:100%;">
<div style=" background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAAGFBMVEUiIiI9PT0eHh4gIB4hIBkcHBwcHBwcHBydr+JQAAAACHRSTlMABA4YHyQsM5jtaMwAAADfSURBVDjL7ZVBEgMhCAQBAf//42xcNbpAqakcM0ftUmFAAIBE81IqBJdS3lS6zs3bIpB9WED3YYXFPmHRfT8sgyrCP1x8uEUxLMzNWElFOYCV6mHWWwMzdPEKHlhLw7NWJqkHc4uIZphavDzA2JPzUDsBZziNae2S6owH8xPmX8G7zzgKEOPUoYHvGz1TBCxMkd3kwNVbU0gKHkx+iZILf77IofhrY1nYFnB/lQPb79drWOyJVa/DAvg9B/rLB4cC+Nqgdz/TvBbBnr6GBReqn/nRmDgaQEej7WhonozjF+Y2I/fZou/qAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;"></div>
</div>
<p style=" margin:8px 0 0 0; padding:0 4px;"> <a href="https://instagram.com/p/067E1nnMN2/" style=" color:#000; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none; word-wrap:break-word;" target="_top">Flash preview: (How) can algorithms be racist? An illustrated story #doodles #datamining #race #partnews</a></p>
<p style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;">A video posted by Sophie C (@petit.chou) on <time style=" font-family:Arial,sans-serif; font-size:14px; line-height:17px;" datetime="2015-04-01T05:31:41+00:00">Mar 31, 2015 at 10:31pm PDT</time></p>
</div>
</blockquote>
<p><script async defer src="//platform.instagram.com/en_US/embeds.js"></script></p>
<p>For this assignment, Alicia and I wanted to tackle the issue of bias and discrimination in algorithms in a creative way. Our response is to this short article from the Guardian, &#8220;<a href="http://www.theguardian.com/commentisfree/2013/feb/05/can-googling-be-racist">Can Googling be Racist?</a>&#8220;.  The Instagram video is a preview of the resulting story, which I plan to scan into a static web-readable series.</p>
<p>To explain, we  supplemented Latanya Sweeney&#8217;s <a href="http://arxiv.org/ftp/arxiv/papers/1301/1301.6822.pdf">research paper</a> with my own knowledge of data mining and algorithms, in a easily-digestable format. One of my biggest gripes as a computer scientist/machine-learner is the assumption that algorithms are either value-free or a mysterious black box. As Mark Twain (might have) said,</p>
<p>&#8220;There are three kinds of lies: lies, damned lies, and statistics.&#8221;</p>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://partnews.mit.edu/2015/04/01/how-can-algorithms-be-racist/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
							</item>
	</channel>
</rss>
