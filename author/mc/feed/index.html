<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>MC &#8211; Future of News and Participatory Media</title>
	<atom:link href="https://partnews.mit.edu/author/mc/feed/" rel="self" type="application/rss+xml" />
	<link>https://partnews.mit.edu</link>
	<description>Treating newsgathering as an engineering problem... since 2012!</description>
	<lastBuildDate>Tue, 28 Jan 2014 16:02:51 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2</generator>
	<item>
		<title>Problems with WikiLeaks and the Start of a Solution</title>
		<link>https://partnews.mit.edu/2013/05/30/problems-with-wikileaks-and-the-start-of-a-solution/</link>
				<pubDate>Thu, 30 May 2013 21:27:23 +0000</pubDate>
		<dc:creator><![CDATA[MC]]></dc:creator>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Final Projects]]></category>

		<guid isPermaLink="false">http://partnews.brownbag.me/?p=2993</guid>
				<description><![CDATA[WikiLeaks has been an important step toward greater transparency and accountability. By publishing leaked documents from governments, corporations and other institutions, WikiLeaks has revealed the power of transparency and openness online. That said, WikiLeaks has some serious flaws with their &#8230; <a href="https://partnews.mit.edu/2013/05/30/problems-with-wikileaks-and-the-start-of-a-solution/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
								<content:encoded><![CDATA[<p><a href="http://wikileaks.org">WikiLeaks</a> has been an important step toward greater transparency and accountability. By publishing leaked documents from governments, corporations and other institutions, WikiLeaks has revealed the power of transparency and openness online. That said, WikiLeaks has some serious flaws with their own transparency, openness of their releases, and methods.</p>
<p>This is an exploration of some of the problems with WikiLeaks through a story of my attempt to analyze the WikiLeaks <a href="http://wikileaks.org/the-gifiles.html">Global Intelligence Files release</a>. The Global Intelligence Files (GI Files) are a collection of five million emails from the intelligence contractor <a href="//www.stratfor.com/”">Stratfor</a>. These emails are from the years 2004 to 2011 and discuss Stratfor’s internal operations.</p>
<p>I was hoping to analyze Stratfor’s <a href="http://estaticos.elmundo.es/documentos/2010/12/01/conspiracies.pdf">communication structures</a> by making a network graph from the GI Files. Email is perfect for this sort of analysis. Someone made a tool called <a href="https://github.com/wlwardiary/cable2graph">cable2graph</a> for graph analysis of <a href="http://www.wikileaks.ch/cablegate.html">Cablegate</a>, so I planned to adapt that tool for use with emails. </p>
<p>Ideally, I would use all the Stratfor emails for this graph analysis. Unfortunately, WikiLeaks has not released all the Global Intelligence Files yet. They started publishing the GI Files on <a href="http://www.youtube.com/watch?v=T8NII5v9keY">February 27th, 2012</a>. This was over a year ago. Five million documents is a lot, but after fifteen months only a fraction of the documents have been released. It does not seem like WikiLeaks is seriously trying to release these documents. They have not released any sets of emails since <a href="http://wikileaks.org/gifiles/releasedate/2013-02-17-00-alpha-more-insight-yemen-awlaki-death-op-sa701.html">February 2013</a> and most months only several sets of emails are published. Even considering time spent reviewing documents before release, the release of the GI Files is taking far too long. </p>
<p>The second issue with analyzing the Global Intelligence Files is WikiLeaks’s release strategy. WikiLeaks is working with <a href="//wikileaks.org/the-gifiles.html”">“more than 25 media partners”</a> to release the documents. These partners get access to the full set of documents. WikiLeaks releases GI Files emails only when a partner writes an article about them. WikiLeaks has released a few hundred sets of these emails so far, but most of these releases only contain a few documents.</p>
<p>This release strategy makes it incredibly difficult to find new stories in released Stratfor emails. After all, the emails are only released when they have been used in a story already. This means there is no obvious way for most WikiLeaks supporters to help with the release and analysis of the GI Files. Partners are <a href="http://cryptome.org/2012/06/wikileaks-trap.htm">invited</a> and must be “﻿a journalist, Professor or Associate Professor at a University or an employee of a human rights organisation.” In some ways, this is worse than a pay wall. Access to the GI Files is restricted and there is no clear way someone can get access.</p>
<p>Despite these difficulties, I thought using network graphs to examine the bigger picture of all the released emails may still reveal some new information. WikiLeaks does very little analysis of most of its documents. For the GI Files, WikiLeaks seems to rely entirely on the analyses done by their media partners. Unfortunately, WikiLeaks makes it difficult to analyze most of their releases. A few, like Cablegate, are accessible in machine readable formats. People have used these formats to <a href="http://www.cabledrum.net/news/redactions">analyze</a> <a href="http://www.cablegatesearch.net/">the</a> <a href="https://github.com/elishowk/cablegate_semnet">documents</a> <a href="http://aebr.home.xs4all.nl/wl/">in</a> <a href="https://github.com/wlwardiary/cable2graph">interesting</a> <a href="https://www.aftenposten.no/spesial/cablegate/?lang=en">ways</a>.</p>
<p>Most WikiLeaks releases are not accessible in machine readable formats. There are no machine readable versions of the Global Intelligence Files. As of this month, the US government has <a href="http://www.whitehouse.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government-">better policies for releasing machine readable data than WikiLeaks</a>. This new policy is a great step forward for the US government, but it is a huge failure for WikiLeaks that they are now behind the US government on certain transparency policies.</p>
<p>To get the GI Files email data in machine readable format, I wrote a <a href="https://scraperwiki.com/scrapers/stratforscraper/">scraper</a>. I was able to scrape the email subjects, dates, IDs, and text. Then I ran into yet another issue. Where the email addresses in the to and from fields should have been in the HTML, there was a <a href="https://www.cloudflare.com/apps/scrapeshield">CloudFlare ScrapeShield</a> script. The purpose of ScrapeShield is to stop spam bots from collecting email addresses from websites. This is a good thing, but ScrapeShield becomes problematic when it gets in the way of analysis of documents. Is it more important that Stratfor employees get less spam or that people can analyze the WikiLeaks documents? I would generally say the latter since extra spam is a minor inconvenience (and most is caught by spam filters), but Stratfor employees have no say in this, so that complicates the situation.</p>
<p>While not ideal, one solution is for WikiLeaks to give only their media partners access to machine readable data or have some method of requesting it. Not only does WikiLeaks not give media partners access to machine readable data, but they <a href="http://cryptome.org/2012/06/wikileaks-trap.htm">actively ban their partners from scraping the documents</a>. This ban on scraping and running scripts greatly limits the types of analysis their partners can conduct. It may also make it more time consuming to find documents to write about where network and content analysis could reveal interesting sets of emails faster. This restrictive partnership system may be why so few of the GI Files emails have been released so far. </p>
<p>Network analysis is not possible without the to and from fields of the emails. I found a way where it may be possible to get the to and from emails by converting the emails to PDFs and then scraping the PDFs, but this would be extremely difficult. PDF scraping itself is hard, but automatically scraping thousands of slightly different PDFs may not be possible. Thus, I gave up on the network analysis idea.</p>
<p>WikiLeaks is not only failing to make it easy to help with document release and analysis, but they actively impede anyone who wants to help, including their own partners. While WikiLeaks has done some great things for transparency, the organization has some serious problems with secrecy. This secrecy spreads to their releases and perpetuates closed documents behind walls. As the purpose of WikiLeaks is encouraging greater transparency and accountability through release of restricted information, restricting access to that information again defeats the purpose.</p>
<p>WikiLeaks is dying and if it does not change its methods it will die. Regardless of what happens, some of the <a href="http://www.collateralmurder.com/">successes</a> <a href="http://en.wikipedia.org/wiki/Information_published_by_WikiLeaks#Kaupthing_Bank">of WikiLeaks</a> have shown the world the power of leaking and transparency. These successful releases are not the norm. Most of the WikiLeaks releases and those of other transparency initiatives are rendered useless by the issues discussed above and others.</p>
<p>We can do better. I am not sure exactly what will work, but there are a few tasks I think a successful solution to these problems will contain. I have been working on a <a href="http://transparencytoolkit.org">transparency platform</a> that addresses the issues described above and other problems I have noticed with leaking and other transparency initiatives.</p>
<p><strong>Define</strong><br />
When examining any leaked or released information, it helps to define what information the investigative group has and what information it needs. There seem to be two main parts to this defining stage and tools that could help with the process, defining investigative questions and steps to answer those questions.</p>
<p>First, it may be helpful to define the questions the group wants to answer about the information. These questions will likely change throughout the process, but defining some questions upfront can help guide the investigation. A platform that lets people post, edit, and add answers to questions is a simple place to start for investigation of released documents. </p>
<p>Second, the group needs to define how they will answer each question. This could mean determining what information they need, how they will collect that information, and what types of analysis they will conduct with the information. Again, these steps may change throughout the process, but a list of clearly defined steps to answer the investigation’s questions may be a good starting point. Clearly defined steps or tasks are also helpful because they make it clear how supporters can help with the investigation. That clear path to involvement alone would be a huge improvement over WikiLeaks where supporters struggle to figure out how they can help. Simple task management software could allow people to define and allocate these steps to answer each question. It may even be possible to suggest steps based on the wording of the question or steps already entered. Suggestions like this would make it easier for people to figure out how to conduct the investigation.</p>
<p><strong>Collect</strong><br />
Some investigations may start around a particular set of documents the group has already. This often seems to be the case with leaking and whistleblowing. In this case, it may be helpful for these documents to be uploaded in one place so people can search and analyze them. These documents should also be uploaded in a machine readable format for analysis. These two goals can be accomplished with a combination of a searchable document storage/upload system and scrapers. </p>
<p>The group examining the released documents may want to collect related information like interviews, data sets, documents released by other organizations, or user contributed data. Or maybe someone has questions and has no documents to start with yet when finding an answer. These additional documents could also be uploaded to the central storage/upload system to make it easier to search, combine, and analyze all the information. This document collection system could go a step further make it easier to find documents with options to pull information from common data sources like government data APIs, Wikipedia, and search results at the click of a button. Additionally, people helping with the investigation could use a browser plugin to easily send documents or scrape web pages they find online to the information storage system.</p>
<p>Sometimes a whistleblower may want to upload related documents anonymously. An <a href="https://globaleaks.org/">anonymous</a> <a href="http://www.newyorker.com/strongbox/">submission system</a> could be adapted to send documents directly to the storage/upload system. There could also be options for automatic redaction of names (or emails) in the documents submitted by whistleblowers (perhaps with a way some people can access the full data). </p>
<p>After all of this information is uploaded, it would be nice if it could be used outside this single investigation. Thus, it could be helpful to give the person uploading documents the option to share them with others using the same document collection system. This sharing system would make it easy to import documents and data into a new investigation or instance of the information collection and storage system.</p>
<p><strong>Analyze</strong><br />
Collecting and releasing documents only goes so far. To use information, people need to understand it. The analysis used to understand the documents includes anything from reading and discussing the documents to combining different forms of information and using content or network analysis programs. The most helpful type of analysis will vary based on the type of information available and questions asked.</p>
<p>Plenty of analysis tools exist, but a toolkit of many different analysis tools (existing and new) that allows these tools to be easily linked together and use information from the document collection system would make them easier to use and more powerful. For example, users could set one tool to parse a set of data from the document collection system and pass the output of that tool to another that does content analysis to determine relevant Wikipedia pages and then have another tool pull dates from those pages. Another tool could then format the dates and the dates in the original data into the format required by TimelineJS and then have TimelineJS with those dates embedded on the analysis/results page. Each of these tools could be used on their own or with different tools as well. Such an analysis toolkit would make existing tools easier to use and make them more powerful by allowing people to hook them together without coding. People who can code could upload their own tools for others to use or modify existing ones.</p>
<p><strong>Release</strong><br />
While the system described above may help with collecting and analyzing information, it may still be too time consuming and complicated for someone who just wants to learn about a situation and not take part in the investigation. With all the questions, documents, and analysis in one place, a program could easily take all of these and reformat them for a release page. This release page could have summaries and basic information on the findings at the top with the full details from the investigation, analysis, and full documents underneath. While the investigation system would be structured to make it easy to contribute, the release page would be structured to make the information easy to read and understand.</p>
<p>No matter how nice the release system and investigation platform is, few people will find the release pages on their own. People need to write articles about the release and share links to the release page. WikiLeaks’s release model of having media partners write articles is not all bad. I think media partners can be a helpful part of a disclosure system so long as documents are not only released when media partners write articles and they have the tools to help with analysis.</p>
<p><strong>Using the Information</strong><br />
Collecting documents, analyzing them, and releasing more understandable information is not helpful if no one uses the information for anything. Using the released information can take many forms, so I am focusing on the first steps mostly for now. That said, the same structure that helps people define questions and steps to answer them could be used to set specific goals for change or greater awareness based on the released information.</p>
<p><strong>Next Steps</strong><br />
Tools exist to help with all of the steps described above. Some people already use these to examine leaked or released documents. Unfortunately, these tools are often difficult to use and identifying and using many different tools or methods quickly becomes cumbersome, so in many cases they are not used at all. I am building the system I described above. This system integrates both existing tools and new ones in a modular and extensible platform for collaborative investigation. </p>
<p>Hopefully this platform will at least make it easier for organizations already releasing or analyzing leaked and released documents to conduct good analyses. Ideally, this system will be directly integrated with sites releasing documents (both leaking organizations and government transparency initiatives) to provide a platform for collaborative investigation and civic engagement between different groups and individuals.</p>
<p><strong>What I Made So Far</strong><br />
I built a prototype of the <a href="http://transparencytoolkitdemo.herokuapp.com/">collaborative investigation platform for defining questions and steps to answer them</a> (minus the automatic suggestions). This also allows people to upload documents, but it does not yet include anything close to the upload system I described. I also made a few small analysis tools to help analyze the Global Intelligence Files. These tools are a <a href="https://scraperwiki.com/scrapers/stratforscraper/">scraper</a> that can pull all the GI Files, specific releases, and single emails, a <a href="//github.com/Shidash/TimelineGen”">gem to automatically generate a TimelineJS-compatible JSON from the scraped emails</a>, and a <a href="http://transparencytoolkitdemo.herokuapp.com/tasks/23">modification of the upload system</a> that embeds a timeline of emails from a single GI Files release when someone uploads the JSON of that release generated by the scraper.</p>
<p>For now, you have to run the scraper <a href="https://scraperwiki.com/scrapers/stratforscraper/">on ScraperWiki</a>, but I hope to integrate it more directly with Transparency Toolkit in the future. The main Transparency Toolkit system can be tested on <a href="http://transparencytoolkitdemo.herokuapp.com/">the demo site</a> (where I’ve uploaded some of the GI Files) or <a href="https://github.com/Shidash/Transparency-Toolkit">downloaded from Github</a>. The TimelineGen gem can be used as a gem or <a href="https://github.com/Shidash/TimelineGen">downloaded from Github</a>. Currently you can only make timelines directly on Transparency Toolkit from specific GI Files release pages, but TimelineGen has methods that can be used in more general cases (I’m still hooking them together manually for the timeline embed). </p>
<p>This is a tutorial/demo video that shows how this works-</p>
<p><iframe width="584" height="329" src="http://www.youtube.com/embed/lBfIMDRc9co?feature=oembed" frameborder="0" allowfullscreen></iframe></p>
<p>If you want to use Transparency Toolkit to make timelines of WikiLeaks GI Files releases, some instructions are below. At this point, it is helpful if you know the basics of how to use ScraperWiki.</p>
<p>1. Go to <a href="//transparencytoolkitdemo.herokuapp.com/”">http://transparencytoolkitdemo.herokuapp.com/</a></p>
<p>2. Ask a question or set a goal by typing in the box at the top (or skip this if you want to add a task to an existing question).</p>
<p>3. Click the + button next to the question to which you want to add a task and add a task by typing in the box that says “Add a task”. Tasks are clearly actionable steps for answering the question, like making a timeline of a set of emails.</p>
<p>4. Go to <a href="http://wikileaks.org/gifiles/releases.html">the GI Files release page</a> and click on one of the links to view a set of emails that was released. Copy the URL of the page with the set of emails.</p>
<p>5. Go to <a href="https://scraperwiki.com/scrapers/stratforscraper/edit/">the GI Files scraper</a> and scroll down to line 99 right under ﻿&#8221;#To get all emails for a single gifiles release:”. Then replace the URL in the getEmail(url) method with the URL you copied in step 4.</p>
<p>6. Save and run the scraper.</p>
<p>7. Click “Back to scraper overview” in the top right corner. Then click the Download menu and choose “As a JSON file”. Be sure to clear the old data before running the scraper again.</p>
<p>8. Go back to the task page you created on Transparency Toolkit in step 3. Click the “Contribute results from task” field and type anything you want about the results.</p>
<p>9. Click the Browse button and select the JSON you downloaded in step 7. Submit the results</p>
<p>10. You should see a timeline of the emails on the release page you specified. You can see an example timeline <a href="//transparencytoolkitdemo.herokuapp.com/tasks/23“">here</a> as well.</p>
]]></content:encoded>
										</item>
		<item>
		<title>Bradley Manning&#8217;s Pretrial Hearing</title>
		<link>https://partnews.mit.edu/2013/04/10/bradley-mannings-pretrial-hearing/</link>
				<pubDate>Wed, 10 Apr 2013 16:53:13 +0000</pubDate>
		<dc:creator><![CDATA[MC]]></dc:creator>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Curated Stories]]></category>

		<guid isPermaLink="false">http://partnews.brownbag.me/?p=2878</guid>
				<description><![CDATA[I made a Storify from tweets of people at Bradley Manning&#8217;s pretial hearing today. You can find it here or below.]]></description>
								<content:encoded><![CDATA[<p>I made a Storify from tweets of people at Bradley Manning&#8217;s pretial hearing today. You can find it <a href="https://storify.com/Shidash/bradley-manning-s-pretrial-hearing">here</a> or below.</p>
<style type="text/css">
       .errordiv { padding:10px; margin:10px; border: 1px solid #555555;color: #000000;background-color: #f8f8f8; width:500px; }#advanced_iframe {visibility:visible;opacity:1;}#ai-layer-div-advanced_iframe p {height:100%;margin:0;padding:0}</style><script type="text/javascript">  var ai_iframe_width_advanced_iframe = 0;  var ai_iframe_height_advanced_iframe = 0;var aiIsIe8=false;var aiOnloadScrollTop="true";
if (typeof aiReadyCallbacks === 'undefined') {
    var aiReadyCallbacks = [];  
} else if (!(aiReadyCallbacks instanceof Array)) {
    var aiReadyCallbacks = [];
}    function aiShowIframeId(id_iframe) { jQuery("#"+id_iframe).css("visibility", "visible");    }    function aiResizeIframeHeight(height) { aiResizeIframeHeight(height,advanced_iframe); }    function aiResizeIframeHeightId(height,width,id) {aiResizeIframeHeightById(id,height);}</script><iframe id="advanced_iframe"  name="advanced_iframe"  src="https://storify.com/Shidash/bradley-manning-s-pretrial-hearing"  width="800"  height="650"  scrolling="auto"  frameborder="0"  allowtransparency="true"  style=";width:800px;height:650px;" ></iframe><script type="text/javascript">var ifrm_advanced_iframe = document.getElementById("advanced_iframe");var hiddenTabsDoneadvanced_iframe = false;
function resizeCallbackadvanced_iframe() {}function aiChangeUrl(loc) {}</script><script type="text/javascript"></script>
]]></content:encoded>
										</item>
		<item>
		<title>Computer Money Going Up (or the rise of the value of Bitcoin)</title>
		<link>https://partnews.mit.edu/2013/04/03/computer-money-going-up-or-the-rise-of-the-value-of-bitcoin/</link>
				<pubDate>Wed, 03 Apr 2013 18:27:17 +0000</pubDate>
		<dc:creator><![CDATA[MC]]></dc:creator>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Explainers]]></category>

		<guid isPermaLink="false">http://partnews.brownbag.me/?p=2772</guid>
				<description><![CDATA[﻿Note: This is an attempt to explain computer money (Bitcoin) going up using the ten hundred most used words (like in up-goer five). These days, a lot of people are talking about computer money. Computer money is money with no &#8230; <a href="https://partnews.mit.edu/2013/04/03/computer-money-going-up-or-the-rise-of-the-value-of-bitcoin/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
								<content:encoded><![CDATA[<p><em></p>
<p>﻿Note: This is an attempt to explain <a href="http://bitcoin.org/en/">computer money</a> (Bitcoin) going up using <a href="http://splasho.com/upgoer5/">the ten hundred most used words</a> (like in <a href="https://xkcd.com/1133/">up-goer five</a>).</p>
<p></em></p>
<p>These days, a lot of people are talking about <a href="http://bitcoin.org/en/">computer money</a>. <a href="http://bitcoin.org/en/">Computer money</a> is money with no group in the middle controlling it. Instead, everyone uses their computer to track each time someone uses computer money. This <a href="https://en.bitcoin.it/wiki/Block_chain">table tracking the use of computer money</a> makes it possible to give others computer money and stops people from lying about how much computer money they have.</p>
<p>People can send computer money to others or get computer money themselves. You <a href="https://en.bitcoin.it/wiki/Secure_Trading">do not need to know</a> who the other person is to send computer money. When computer money is used, the person getting the money tells the person giving them computer money a number to send it to. This number can be a different number every time so no one knows who gets the computer money. The person giving away the computer money uses another number they do not tell anyone to show the computer money is real and has not been changed. The computers of the other people using computer money then check to be sure the money is real and add the use of computer money to their tables.</p>
<p>More computer money is made <a href="https://en.bitcoin.it/wiki/Mining">when computers get hard problems right</a>. A computer somewhere in the world finds the answer to one of these hard problems about every ten minutes. Many strong computers work on these problems, so it is hard to get computer money this way. The person who owns the computer that gets the problem right also gets the computer money. The number of computer money given for finding the answer to a problem goes down over time.</p>
<p>Computer money can be used to buy other types of money. For the past few weeks, the number of other types of money one piece of computer money can buy <a href="http://finance.yahoo.com/blogs/daily-ticker/bitcoin-prices-blast-100-driving-speculators-wild-150415225.html">has been going up a lot</a>. This makes some people worried that the pieces of other types of money one piece of computer money can buy will <a href="http://www.businessinsider.com/why-bitcoin-is-like-no-other-bubble-weve-seen-before-2013-4">drop very low soon</a>. </p>
<p>Why are the pieces of other types of money one piece of computer money can buy going up so much? There are a few possible reasons. <a href="http://www.itworld.com/it-management/350793/online-electronics-retailer-has-successful-trial-bitcoin-only-transactions">More places</a> are starting to accept computer money. Money is only money if it can be used to buy things. The more things money can buy, the more it can be used. Since more places accept computer money, <a href="http://www.ibtimes.com/bitcoins-accepted-payment-expensify-virtual-cash-surpasses-value-20-national-currencies-1160905">more people are interested in using computer money</a>.</p>
<p>As more people started using computer money, one state started to make people who use their computers to make computer money or <a href="https://mtgox.com/">help others</a> use computer money <a href="http://www.fincen.gov/statutes_regs/guidance/pdf/FIN-2013-G001.pdf">tell them how they use the computer money</a>. This is to stop people from <a href="http://en.wikipedia.org/wiki/Money_laundering">hiding that they get money from places not allowed by the state</a>.</p>
<p>At the same time, <a href="http://www.marketplace.org/topics/world/whiteboard/what-just-happened-cyprus-explainer">a different state needed to give another a lot of money</a>. This state had to take money from the people who live there to pay the money back to the places they got it from. People in other states were worried about the same thing happening to them. This made computer money, which does not need a state in the middle controlling it, <a href="http://www.forbes.com/sites/petercohan/2013/04/02/are-bitcoins-safer-than-cyprus/">look good</a>. </p>
<p>All of this caused computer money to get more attention. As computer money got more attention, more people started accepting it. One person even tried to <a href="http://rt.com/news/house-for-sale-bitcoins-781/">offer his house for computer money</a>. This attention, in turn, made computer money easier to use. When computer money is easier to use, the pieces of other types of money one piece of computer money can buy goes up. Now, all of the computer money can be used to buy <a href="http://rt.com/news/bitcoin-challenge-dollar-currency-121/">more than ten hundred ten hundred ten hundred pieces of some other types of money</a>.</p>
]]></content:encoded>
										</item>
		<item>
		<title>Fact Checking the Westboro Baptist Church</title>
		<link>https://partnews.mit.edu/2013/03/13/wbcfactcheck/</link>
				<pubDate>Wed, 13 Mar 2013 17:39:28 +0000</pubDate>
		<dc:creator><![CDATA[MC]]></dc:creator>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Fact-checking Assignments]]></category>

		<guid isPermaLink="false">http://partnews.brownbag.me/?p=2420</guid>
				<description><![CDATA[I fact checked the Westboro Baptist Church&#8217;s blog and used Bounce to annotate it. My annotations are here.]]></description>
								<content:encoded><![CDATA[<p>I fact checked the <a href="http://blogs.sparenot.com/">Westboro Baptist Church&#8217;s blog</a> and used <a href="http://bounceapp.com/">Bounce</a> to annotate it. My annotations are <a href="http://bounceapp.com/89852">here</a>.</p>
]]></content:encoded>
										</item>
		<item>
		<title>Interview with Erhardt Graeff</title>
		<link>https://partnews.mit.edu/2013/03/06/interview-with-erhardt-graeff/</link>
				<pubDate>Wed, 06 Mar 2013 18:29:56 +0000</pubDate>
		<dc:creator><![CDATA[MC]]></dc:creator>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Classmate Profiles]]></category>

		<guid isPermaLink="false">http://partnews.brownbag.me/?p=2298</guid>
				<description><![CDATA[For the interview assignment, I interviewed Erhardt Graeff. Below is an edited clip from the interview with Erhardt introducing himself and discussing his research interests, how he became interested in the area, his future plans, and why he is taking &#8230; <a href="https://partnews.mit.edu/2013/03/06/interview-with-erhardt-graeff/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
								<content:encoded><![CDATA[<p>For the interview assignment, I interviewed Erhardt Graeff. Below is an edited clip from the interview with Erhardt introducing himself and discussing his research interests, how he became interested in the area, his future plans, and why he is taking this class.</p>
<p><a href="http://partnews.brownbag.me/2013/03/06/interview-with-erhardt-graeff/interviewclip/" rel="attachment wp-att-2299">Interview Audio Clip</a></p>
<p>I also made a timeline with some of the events in Erhardt&#8217;s life. You can view this below or <a href="http://www.shidash.com/erhardt.html">here</a>.</p>
<style type="text/css">
       .errordiv { padding:10px; margin:10px; border: 1px solid #555555;color: #000000;background-color: #f8f8f8; width:500px; }#advanced_iframe_2 {visibility:visible;opacity:1;}#ai-layer-div-advanced_iframe_2 p {height:100%;margin:0;padding:0}</style><script type="text/javascript">  var ai_iframe_width_advanced_iframe_2 = 0;  var ai_iframe_height_advanced_iframe_2 = 0;var aiIsIe8=false;var aiOnloadScrollTop="true";
if (typeof aiReadyCallbacks === 'undefined') {
    var aiReadyCallbacks = [];  
} else if (!(aiReadyCallbacks instanceof Array)) {
    var aiReadyCallbacks = [];
}    function aiShowIframeId(id_iframe) { jQuery("#"+id_iframe).css("visibility", "visible");    }    function aiResizeIframeHeight(height) { aiResizeIframeHeight(height,advanced_iframe_2); }    function aiResizeIframeHeightId(height,width,id) {aiResizeIframeHeightById(id,height);}</script>Http iframes are not shown in https pages in many major browsers. Please read <a href="http://www.tinywebgallery.com/blog/iframe-do-not-mix-http-and-https" rel="nofollow" target="_blank">this post</a> for details.<iframe id="advanced_iframe_2"  name="advanced_iframe_2"  src="http://embed.verite.co/timeline/?source=0Akk1thccTm5fdG1CSFZYX2FPUU1yRFBYNjhnUmNFYUE&#038;font=Bevan-PotanoSans&#038;maptype=toner&#038;lang=en&#038;height=650"  width="800"  height="650"  scrolling="auto"  frameborder="0"  allowtransparency="true"  style=";width:800px;height:650px;" ></iframe><script type="text/javascript">var ifrm_advanced_iframe_2 = document.getElementById("advanced_iframe_2");var hiddenTabsDoneadvanced_iframe_2 = false;
function resizeCallbackadvanced_iframe_2() {}function aiChangeUrl(loc) {}</script><script type="text/javascript"></script>
]]></content:encoded>
										</item>
		<item>
		<title>4 Hour Challenge: Cory Doctorow&#8217;s Book Tour</title>
		<link>https://partnews.mit.edu/2013/02/27/4-hour-challenge-cory-doctorows-book-tour/</link>
				<comments>https://partnews.mit.edu/2013/02/27/4-hour-challenge-cory-doctorows-book-tour/#comments</comments>
				<pubDate>Wed, 27 Feb 2013 03:58:04 +0000</pubDate>
		<dc:creator><![CDATA[MC]]></dc:creator>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Classmate Profiles]]></category>
		<category><![CDATA[Four Hour Challenges]]></category>

		<guid isPermaLink="false">http://partnews.brownbag.me/?p=1998</guid>
				<description><![CDATA[I have admired Cory Doctorow’s books for a few years now. He has an amazing ability to weave simple explanations of technical concepts into rallying-cry stories. Doctorow recently released a new book, Homeland, and is on tour with the book. &#8230; <a href="https://partnews.mit.edu/2013/02/27/4-hour-challenge-cory-doctorows-book-tour/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
								<content:encoded><![CDATA[<p><a href="http://partnews.brownbag.me/2013/02/27/4-hour-challenge-cory-doctorows-book-tour/img_20130226_192043_704/" rel="attachment wp-att-1999"><img class="alignnone size-medium wp-image-1999" src="http://partnews.brownbag.me/wp-content/uploads/2013/02/IMG_20130226_192043_704-168x300.jpg" alt="" width="168" height="300" srcset="https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_192043_704-168x300.jpg 168w, https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_192043_704-576x1024.jpg 576w" sizes="(max-width: 168px) 100vw, 168px" /></a></p>
<p>I have admired Cory Doctorow’s books for a few years now. He has an amazing ability to weave simple explanations of technical concepts into rallying-cry stories. Doctorow recently released a new book, <em>Homeland</em>, and is on tour with the book. I went to his talk in Cambridge today. He is every bit as good a speaker as he is a writer.</p>
<p><span id="more-1998"></span></p>
<p>The room was packed. When I first came in, I could not even see Doctorow because people spilled around the bookshelves. As the crowd shifted, I eventually got to a place where I could see the talk and take pictures.</p>
<p><a href="http://partnews.brownbag.me/2013/02/27/4-hour-challenge-cory-doctorows-book-tour/img_20130226_192249_157/" rel="attachment wp-att-2000"><img class="alignnone size-medium wp-image-2000" src="http://partnews.brownbag.me/wp-content/uploads/2013/02/IMG_20130226_192249_157-300x168.jpg" alt="" width="300" height="168" srcset="https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_192249_157-300x168.jpg 300w, https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_192249_157-1024x576.jpg 1024w, https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_192249_157-500x281.jpg 500w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Doctorow started by talking about Aaron Swartz (who helped Doctorow with ideas for <em>Homeland</em> and wrote and afterword), from the time he met him to his death. Much of this part of the talk is covered in <a href="http://boingboing.net/2013/01/12/rip-aaron-swartz.html">this post</a>. I have recounted the major points below.</p>
<p>Doctorow went on to explain the importance of case law accessible through PACER. While it is important precedent and not copyrighted, the legal documents still cost money to access. In 2008, Aaron Swartz used RECAP, a tool for posting PACER documents freely, to post 20% of US case law documents online. While technically legal, the FBI was not happy with Swartz, so they put him under surveillance and approached him. Luckily, everything turned out fine that time.</p>
<p>But then, as Doctorow recounted, Aaron Swartz turned to JSTOR in 2010. Access to many documents on JSTOR is limited to people who pay for a subscription or the article. Unfortunately, Doctorow explained, this can limit innovation and progress. He recounted a story of a teenager who came up with a potentially life-saving idea for an early stage test for pancreatic cancer by reading articles downloaded from JSTOR.</p>
<p>So Swartz used a script to download articles from JSTOR, which he had access to legally.  The prosecutors saw this as stealing. Doctorow, however, compared this action to “checking too many books out of the library”. Swartz decided to fight the charges.</p>
<p>While this case was ongoing, many organizations were fighting against <a href="http://en.wikipedia.org/wiki/Stop_Online_Piracy_Act">Stop Online Piracy Act (SOPA)</a> and related bills. The problem with these bills is that they could shut down sites where users can post content by requiring the owners of those websites to be sure posts do not link to copyright infringing material.This would shut down a large portion of the Internet. So the Internet fought back.</p>
<p>Doctorow then described how one of the most effective tools for getting people to campaign against SOPA worked. “There was a tool that we helped build that would allow you to put up a little widget on your website that when people came to visit it would say ‘Oh, hi, you like my website. I’m gonna have to shut it down if this stupid law passes. Uh, what’s your zipcode? Oh, yeah, so here’s your congressmen and here’s your senators and here’s where they stand on this. Do you want to give them a phone call? Because you can do that by clicking this button. 8 million phone calls. And congress realized that while it’s hard to get reelected without campaign finance, it’s much harder to get reelected without votes.”</p>
<p>Doctorow then moved to how all of these issues connect to the bigger picture and the future. “Aaron didn’t do any of this because information wants to be free,” he explained, “Information and I, we had a long, soulful heart to heart and it confided in me that the only thing it really wants from any of us is for us to stop anthropomorphizing it. Cause information ‘wants’ nothing, but I’ll tell you what people want, and the may you make people free is sometimes with good information. If you know the law, you are more free than someone who doesn’t know the law. If you know the truth of the world, you are more free than someone who doesn’t know the truth.”</p>
<p>He echoed this idea about the power of information to improve the lives of people later in the talk when, in response to a question about why we should care about free information when kids are starving, Doctorow referenced a study where poor people who were given Internet access had a significantly higher quality of life than those who did not have Internet access.</p>
<p>One part of making people free is ensuring that everyone has control over the devices they own and those that influence their lives because “the Internet and computers are what our world is made of today. But we don’t regulate them like they’re important. We regulate them like they’re a fact machine attached to a waffle iron. We regulate them like they are the second coming of telephony&#8230;. and not like they are the nervous system of the 21st century where everything we do requires the Internet. We get it terribly, horribly wrong.” One of the major reasons why we get this regulation wrong is because of the influence of money on politics and laws.</p>
<p>Doctorow said that in <em>Homeland</em>, he wanted someone to get elected as an independent without major donors. To understand how this might be realistically done, Doctorow emailed people experienced with various campaigns. Aaron Swartz replied with a description of a vote-getting machine. Doctorow put it directly into the book.</p>
<p>But what might happen in a future where we keep getting the regulation of the Internet and devices completely wrong? Doctorow discussed a future hearing aid that would be implanted and control what we heard or did not hear. This could clearly be misused. “Lest you feel that is a far-fetched, science-fictiony type of idea,” Doctorow cautioned, “it’s actually a toned down version of reality as it already is.” He then described pace makers that could be remote controlled and remotely update other pacemakers. This is the beauty of Doctorow’s ideas and writings. He warns people about the future by using technology and policy today as an example.</p>
<p>“We are making our future today,&#8221; Doctorow continued, “It’s the beginning of the future. And it will either be a future where the computers listen to us and do what we say where their default posture is ‘yes, master’ or it will be a future where the computers don’t let us do what we want, where they give us orders, where their default posture is ‘I’m sorry, I can’t let you do that Dave’. And we get to choose.” But, as Swartz said in his afterword to Homeland, “It only works if you take part.”</p>
<p>After the main part of his talk, Doctorow took a moment to discuss suicide and depression. “Whatever problems Aaron was facing, killing himself didn’t solve them,” he said, “whatever problems Aaron was facing, they will go unsolved forever.” But on a lighter note, Doctorow pointed out that the Internet and computers could be some of the best tools yet to help us take care of each other so long as we stop to ask how people are feeling.</p>
<p><a href="http://partnews.brownbag.me/2013/02/27/4-hour-challenge-cory-doctorows-book-tour/img_20130226_201909_690/" rel="attachment wp-att-2001"><img class="alignnone size-medium wp-image-2001" src="http://partnews.brownbag.me/wp-content/uploads/2013/02/IMG_20130226_201909_690-300x168.jpg" alt="" width="300" height="168" srcset="https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_201909_690-300x168.jpg 300w, https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_201909_690-1024x576.jpg 1024w, https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_201909_690-500x281.jpg 500w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p><a href="http://partnews.brownbag.me/2013/02/27/4-hour-challenge-cory-doctorows-book-tour/img_20130226_203623_477/" rel="attachment wp-att-2002"><img class="alignnone size-medium wp-image-2002" src="http://partnews.brownbag.me/wp-content/uploads/2013/02/IMG_20130226_203623_477-300x168.jpg" alt="" width="300" height="168" srcset="https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_203623_477-300x168.jpg 300w, https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_203623_477-1024x576.jpg 1024w, https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_203623_477-500x281.jpg 500w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>After that, Doctorow took a few questions on topics related to his talk and people lined up to get their books signed. Artisan’s Asylum was there talking to people as well.</p>
<p>Overall, it was an inspiring and fascinating talk that covered a broad array of issues we are all facing today. This description does not do it justice.</p>
<p>Oh, and I got my book signed.</p>
<p><a href="http://partnews.brownbag.me/2013/02/27/4-hour-challenge-cory-doctorows-book-tour/img_20130226_224419_327/" rel="attachment wp-att-2003"><img class="alignnone size-medium wp-image-2003" src="http://partnews.brownbag.me/wp-content/uploads/2013/02/IMG_20130226_224419_327-300x168.jpg" alt="" width="300" height="168" srcset="https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_224419_327-300x168.jpg 300w, https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_224419_327-1024x576.jpg 1024w, https://partnews.mit.edu/wp-content/uploads/2013/02/IMG_20130226_224419_327-500x281.jpg 500w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
]]></content:encoded>
							<wfw:commentRss>https://partnews.mit.edu/2013/02/27/4-hour-challenge-cory-doctorows-book-tour/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
							</item>
		<item>
		<title>The Future of News, Civic Engagement, and Everything</title>
		<link>https://partnews.mit.edu/2013/02/25/the-future-of-news-civic-engagement-and-everything/</link>
				<comments>https://partnews.mit.edu/2013/02/25/the-future-of-news-civic-engagement-and-everything/#comments</comments>
				<pubDate>Mon, 25 Feb 2013 23:45:51 +0000</pubDate>
		<dc:creator><![CDATA[MC]]></dc:creator>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Elements of Journalism Reflections]]></category>

		<guid isPermaLink="false">http://partnews.brownbag.me/?p=1945</guid>
				<description><![CDATA[The future of news is tied directly to the future of how citizens are informed. This connection is confirmed by Starr and Kovach and Rosenstiel. I would go a step further and say that the news is connected to civic &#8230; <a href="https://partnews.mit.edu/2013/02/25/the-future-of-news-civic-engagement-and-everything/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
								<content:encoded><![CDATA[<p>The future of news is tied directly to the future of how citizens are informed. This connection is confirmed by <a href="http://ethanzuckerman.com/reading/starrchp3.pdf">Starr</a> and <a href="http://ethanzuckerman.com/reading/elements%20of%20journalism.pdf">Kovach and Rosenstiel</a>. I would go a step further and say that the news is connected to civic engagement and participation. This is increasingly true as participatory and social media provide a way for anyone to join the discussion of events and articles. I think this trend towards greater participation and engagement with the news is a good thing and should continue, but I also think there is a place for traditional media.</p>
<p>Participatory media alone has two opposite problems; either there is so much information on an subject that it is impossible to fully understand the discussion and topic itself or not enough people talk about and analyze a subject. I think traditional media serves two key roles to balance out a mostly participatory news ecosystem. First, traditional media synthesizes and distills the discussions in participatory media and projects this information to a broader audience. Second, traditional media serves as a guidepost for what topics are important for citizens to discuss and investigate.</p>
<p>The traditional media is still in an incredibly powerful and potentially dangerous role. If anything, it is more powerful than before as citizens intentionally or unintentionally base their own increasingly spread beliefs on what they hear in the traditional media. As a result, it needs to be handled carefully. One example of an issue highlighted by Kovach and Rosenstiel is the corporate influence of news. We need to develop ways of minimizing and disclosing these external influences if traditional media is to have its ideas spread throughout participatory media.</p>
<p>Just as participatory media is balanced by traditional, this issue with traditional media can be tempered by participatory media. In the early stages of a story, journalists can listen to the conversations in participatory media, understand them, and incorporate them in the story. We need to make this balancing cycle easier by developing tools and fostering collaboration between professional journalists and citizens.</p>
<p>Aaron Swartz proposed an interesting solution for the future of <a href="http://www.aaronsw.com/weblog/usefultransparency">transparency</a> and <a href="http://www.aaronsw.com/weblog/ist">news</a>. He suggested that journalists, bloggers, programmers, lobbyists, and people with all sorts of skills work together in investigative strike teams to understand and fix society&#8217;s problems. I think this is a great way for traditional and participatory media to benefit from each other in a way that results in not only increased access to information but also tangible improvements to the world.</p>
<p>I also think this is happening naturally in many ways. Leaking organizations are one example of this process. Many leaking organizations are independent institutions run by normal citizens who receive and verify information, find background and supplementary information, analyze documents, and work with traditional media partners to release and explain the information. At their best, leaking organizations work very much like Swartz’s investigative strike teams. We need to encourage investigative strike team-like partnerships, teach people how to make participatory media, and build structures to make it easier to get involved and understand all the information available. I have one specific proposal for doing that in leaking <a href="http://shidash.com/toolkitdesc.html">here</a>.</p>
]]></content:encoded>
							<wfw:commentRss>https://partnews.mit.edu/2013/02/25/the-future-of-news-civic-engagement-and-everything/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
							</item>
		<item>
		<title>MC&#8217;s Media Diary</title>
		<link>https://partnews.mit.edu/2013/02/14/mcs-media-diary/</link>
				<comments>https://partnews.mit.edu/2013/02/14/mcs-media-diary/#comments</comments>
				<pubDate>Thu, 14 Feb 2013 06:23:51 +0000</pubDate>
		<dc:creator><![CDATA[MC]]></dc:creator>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Media Diaries]]></category>

		<guid isPermaLink="false">http://partnews.brownbag.me/?p=1573</guid>
				<description><![CDATA[Tracking media consumption is hard. We are constantly inundated with ads, TVs and music in the background, pictures, and other bits of media we may barely see. Sites with dynamic content, like Facebook and Twitter are particularly hard to track. &#8230; <a href="https://partnews.mit.edu/2013/02/14/mcs-media-diary/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
								<content:encoded><![CDATA[<p>Tracking media consumption is hard. We are constantly inundated with ads, TVs and music in the background, pictures, and other bits of media we may barely see. Sites with dynamic content, like Facebook and Twitter are particularly hard to track. The content is varied and there is no lasting record of content viewed. This is problematic because I get a significant portion of my news through Twitter. I could track every tweet I load, but I do not read every tweet I load. The same goes for articles on a webpage with ads or multiple types of media on the page. I might not read the ads or the comments, but there is no way to automatically tell. Gaze tracking on the page may be one way to solve this problem.</p>
<p>It is probably possible to track media consumption well with some elaborate scheme and the right software. I have some ideas of how to do this, but I mostly stuck to tracking sites I visited on my computer plus the more major offline media consumption experiences. I also focused on content I consumed because, while I did produce content, I did not track time spent consuming or creating different types of media. While I spent a significant amount of time creating content, the number of things I made is insignificant when compared to the number I consumed.</p>
<p>I manually categorized the few thousand individual pages I visited and some offline media experiences in the past week. The graphs and discussions of each graph are below.</p>
<p><a href="http://partnews.brownbag.me/2013/02/14/mcs-media-diary/viewon/" rel="attachment wp-att-1862"><img class="alignnone size-full wp-image-1862" src="http://partnews.brownbag.me/wp-content/uploads/2013/02/viewon.png" alt="" width="285" height="224" /></a></p>
<p>The above graph shows how I consume media. As I was primarily tracking links, I naturally consume most of my media on my computer. I also use my phone to quickly look things up and read the news while in transit. If I was fully able to track my Twitter usage, my phone percentage would likely be higher.</p>
<p>At 0.5%, offline consumption is barely visible. Offline consumption includes paper books and handouts, classes, and lectures. Conversations, ads I see, music or TV programs in the background of a room, and other media I consume intentionally or accidentally would greatly increase my offline consumption. Unfortunately, I did not track all of those.</p>
<p>This graph highlights the limitations in my tracking method. That said, I do spend much of my time in front of my computer or phone visiting links. So what types of media do I consume?</p>
<p><a href="http://partnews.brownbag.me/2013/02/14/mcs-media-diary/mediatype/" rel="attachment wp-att-1863"><img class="alignnone size-medium wp-image-1863" src="http://partnews.brownbag.me/wp-content/uploads/2013/02/mediatype-300x214.png" alt="" width="300" height="214" srcset="https://partnews.mit.edu/wp-content/uploads/2013/02/mediatype-300x214.png 300w, https://partnews.mit.edu/wp-content/uploads/2013/02/mediatype-420x300.png 420w, https://partnews.mit.edu/wp-content/uploads/2013/02/mediatype.png 485w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Apparently I do a lot of searches. My searches category also includes searches on individual websites, but 33.2% of all pages I view is a lot. I did not track the content of my searches, but it probably is proportioned similarly to the other categories.</p>
<p>Another interesting finding is that I read more blogs than standard articles. I also view many school website pages as I check hours of food places, read assignments, upload school work, and check course registration.</p>
<p>There are a few things this tracks poorly. Books and TV and movies have small slices because I consumed relatively few of them. In some ways, the less time a particular type of content takes to consume, the more I view it. Thus, the most time consuming activities appear far less prominently than they should. For a different reason, social networks should have a bigger slice. Tweets do not take long to read, so I read a lot of them, but each tweet does not add a page view.</p>
<p><a href="http://partnews.brownbag.me/2013/02/14/mcs-media-diary/contentgraph/" rel="attachment wp-att-1864"><img class="alignnone size-medium wp-image-1864" src="http://partnews.brownbag.me/wp-content/uploads/2013/02/contentgraph-300x269.png" alt="" width="300" height="269" srcset="https://partnews.mit.edu/wp-content/uploads/2013/02/contentgraph-300x269.png 300w, https://partnews.mit.edu/wp-content/uploads/2013/02/contentgraph-333x300.png 333w, https://partnews.mit.edu/wp-content/uploads/2013/02/contentgraph.png 463w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>The graph above shows the type of content I consume. The categories are based on which areas viewed the most media about this week. After searches and social media, I consume the most media about free information. Free information is a category I created to include transparency, open access, leaking and disclosure, and other related areas. For most people, free info would probably not be a category. Instead, they might read a couple related articles a week in US and world news. These categories are just what worked for me.</p>
<p>Likewise, I am not sure there is ever a normal or average week in content or type of media I consume. Some people have discussed events, like the snow storm, that make their media consumption this week abnormal. I definitely looked at more weather pages than I generally do. That said, many weeks there is a story I search for many articles on and cross reference so I can understand the whole situation. This constant searching for many articles drastically and regularly distorts what type of content I read.</p>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://partnews.mit.edu/2013/02/14/mcs-media-diary/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
							</item>
	</channel>
</rss>
